{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjayz/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from dataset import get_valid_chinese_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "bert_hanzis = get_valid_chinese_chars(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the characters into train, val, test\n",
    "index = list(range(len(bert_hanzis)))\n",
    "random.shuffle(index)  # shuffle the index\n",
    "train_split = int(len(bert_hanzis) * 0.8)  # 80% for train\n",
    "val_split = int(len(bert_hanzis) * 0.9)  # 10% for val\n",
    "train_df = pd.DataFrame({'hanzi': bert_hanzis[:train_split]})\n",
    "val_df = pd.DataFrame({'hanzi': bert_hanzis[train_split:val_split]})\n",
    "test_df = pd.DataFrame({'hanzi': bert_hanzis[val_split:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['image_name'] = train_df['hanzi'] + '.jpg'\n",
    "val_df['image_name'] = val_df['hanzi'] + '.jpg'\n",
    "test_df['image_name'] = test_df['hanzi'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['image_name'].to_csv('./data/train.csv', index=False)\n",
    "val_df['image_name'].to_csv('./data/val.csv', index=False)\n",
    "test_df['image_name'].to_csv('./data/test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Chinese Character Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from tqdm import tqdm\n",
    "from pathlib2 import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_name = 'STSONG'\n",
    "font_tft_path = f'./data/font/{font_name}.TTF'\n",
    "text_size = 64 # also the image size\n",
    "font = ImageFont.truetype(font_tft_path, text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hanzi_image(characters, text_size, font, save_dir):\n",
    "    if not save_dir.exists():\n",
    "        save_dir.mkdir(parents=True)\n",
    "    for character in tqdm(characters):\n",
    "        text_width, text_height = font.getsize(character)\n",
    "        xmin, ymin, xmax, ymax = font.getmask(character).getbbox()\n",
    "        offsetx, offsety = font.getoffset(character)\n",
    "        canvas = Image.new('L', (text_width, text_height), (255))\n",
    "        draw = ImageDraw.Draw(canvas)\n",
    "        white = 0\n",
    "        draw.text((0, 0), character, font=font, fill=white)\n",
    "        x_gap = (text_size - (xmax - xmin)) / 2\n",
    "        y_gap = (text_size - (ymax - ymin)) / 2\n",
    "        canvas = canvas.crop((xmin + offsetx - x_gap, ymin + offsety - y_gap, xmax + offsetx + x_gap, ymax + offsety + y_gap))\n",
    "        # check the size of canvas\n",
    "        assert canvas.size == (text_size, text_size)\n",
    "        canvas.save(str(save_dir / '{}.jpg'.format(character)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5610 [00:00<?, ?it/s]/tmp/ipykernel_7045/1109264428.py:5: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  text_width, text_height = font.getsize(character)\n",
      "/tmp/ipykernel_7045/1109264428.py:7: DeprecationWarning: getoffset is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox instead.\n",
      "  offsetx, offsety = font.getoffset(character)\n",
      "100%|██████████| 5610/5610 [00:03<00:00, 1441.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dir = Path('./data/hanzi_img') / font_name\n",
    "generate_hanzi_image(bert_hanzis, text_size, font, dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a7ad60629c7992fdadfcbec1ee9d18d5ed6e222b8ad46df459875a57fae42b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
